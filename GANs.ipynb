{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GANs.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNphSTFnEuFPAv52KY1syMO"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"c39UEUYc8ZJh"},"source":["# Generative Adversarial Networks (GANs)\n","\n","Generative models that **create new data instances that resemble the training data**.\n","\n","```\n","For example, GANs can create images that look like photographs of human faces, \n","even though the faces don't belong to any real person.\n","```\n","\n","Process:\n","* Pairing a generator with a discriminator:\n","  * *Generator* learns to produce the target output\n","  * *Discriminator*: learns to distinguish true data from the output of the generator. \n","* The generator tries to fool the discriminator, and the discriminator tries to keep from being fooled."]},{"cell_type":"markdown","metadata":{"id":"fCM3ek2b9KWt"},"source":["## Generative Models\n","\n","* Generative models \n","  * can generate new data instances.\n","  * capture the joint probability p(X,Y), or just p(X) if there are no labels.\n","  * includes the distribution of the data itself, and determines how likely a given instance is.\n","* Discriminative models \n","  * discriminate between different kinds of data instances.\n","  * capture the conditional probability p(Y|X).\n","  * ignores the question of whether a given instance is likely, and just determines how likely a label is to apply to the instance.\n","\n","### Modeling Probabilities\n","Both generative and discriminative models can estimate probabilities, but they don't have to.\n","\n","* Generative models\n","  * It can model the distribution of data by imitating that distribution.\n","  * It produces convincing \"fake\" data that looks like it's drawn from that distribution.\n","* Disciriminative models\n","  * It can also label an instance without assigning a probability to that label.\n","  * It produces predicted labels with distribution similar to the real distribution of labels in the data.\n","\n","### Generative Models Are Hard\n","* Generative models \n","  * Have to model more.\n","  * It might capture correlations of objects (in an image) other than the main object. \n","  * They try to model how data is placed throughout the space produces very complicated distributions.\n","* Discriminative models \n","  * Have to model relatively less. \n","  * It might capture the difference between the objects (in an image) by just looking for a few tell-tale patterns. It could ignore many of the correlations that the generative model must get right.\n","  * They try to draw boundaries in the data space.\n","\n","**GANs offer an effective way to train such rich models to resemble a real distribution**."]},{"cell_type":"markdown","metadata":{"id":"YoNGXCwpwad6"},"source":["## GAN Structure\n","\n","### Generator \n","* It is a neural network.\n","* Learns to generate plausible data.\n","* The output is connected directly to the discriminator input.  \n","* The generated instances become negative training examples for the discriminator.\n","* Generator training\n","  * Random input (like noise or a uniform distribution)\n","  * Generator network, which transforms the random input into a data instance\n","  * Discriminator network, which classifies the generated data\n","discriminator output as real or fake.\n","  * Calculate *generator loss*, which penalizes the generator for failing to fool the discriminator.\n","  * Backpropagate through both the discriminator and generator to obtain gradients.\n","  * Use gradients to change only the generator weights.\n","* It creates fake data by incorporating feedback from the discriminator.\n","  * Through backpropagation, the discriminator's classification provides a signal that the generator uses to update its weights.\n","\n","### Discriminator \n","* It is a neural network.\n","* The input comes from two sources:\n","  * Fake data directly from the generator output; used as negative examples.\n","  * Real data instances; used as positive examples.\n","* Learns to distinguish the fake data from real data.\n","  * Classifies the real data.\n","  * Classifies the fake data from the generator.\n","  * Ignores the generator loss and just uses the *discriminator loss*.\n","  * The *discriminator loss* penalizes the discriminator for misclassifying a real instance as fake or a fake instance as real.\n","    * It updates its own weights through backpropagation.\n","* It then penalizes the generator for producing implausible results.\n","  * Through backpropagation, it provides a signal that the generator uses to update its weights."]},{"cell_type":"markdown","metadata":{"id":"_gUeKXLw13yZ"},"source":["## GAN Training\n","\n","```\n","1. When training begins, the generator produces fake data.\n","2. The discriminator quickly learns to tell that it's fake.\n","3. As training progresses, the generator gets closer to producing output closer to real data.\n","4. The discriminator starts to get fooled. If the training goes well, the discriminator gets worse at telling the difference between real and fake. \n","5. The discriminator starts to classify fake data as real, and its accuracy decreases.\n","```\n","\n","Because a GAN contains two separately trained networks, its training algorithm must address two complications:\n","* GANs must juggle two different kinds of training (generator and discriminator).\n","* GAN convergence is hard to identify\n","\n","### Alternating Training\n","1. The discriminator trains for one or more epochs.\n","  * Keep the generator constant during the discriminator training phase.\n","  * Learns how to recognize the generator's flaws.\n","2. The generator trains for one or more epochs.\n","  * Keep the discriminator constant during the generator training phase.\n","  * Optimize based on discriminator back-propagated signal.\n","3. Repeat steps 1 and 2 to continue to train the generator and discriminator networks.\n","\n","### Convergence\n","* As the generator improves with training, the discriminator performance gets worse because the discriminator can't easily tell the difference between real and fake. \n","* If the generator succeeds perfectly, then the discriminator has a 50% accuracy. <br>`In effect, the discriminator flips a coin to make its prediction.`\n","* The discriminator feedback gets less meaningful over time. \n","  * This progression poses a problem for convergence of the GAN as a whole.\n","* The convergence is often a fleeting, rather than stable, state.\n","  * If the GAN continues training past the point when the discriminator is giving completely random feedback, and the generator starts to train on junk feedback, and its own quality may collapse."]},{"cell_type":"markdown","metadata":{"id":"-B_YbB-R3sLX"},"source":["## GAN Loss Functions\n","* GANs try to replicate a probability distribution. \n","* The loss function needs to reflect the distance between the distribution of the data generated by the GAN and that of the real data.\n","* A GAN can have two loss functions: \n","  * one for generator training\n","  * one for discriminator training.\n","* The generator can only affect one term in the distance measure that reflects the distribution of the fake data. So, during generator training we drop the other term which reflects the distribution of the real data.\n","\n","### Minimax Loss\n","\n","The **generator tries to minimize** the following function while the **discriminator tries to maximize** it. It derives from the cross-entropy between the real and generated distributions.\n","\n","*E<sub>x</sub>[ log(D(x)) ] + E<sub>z</sub>[ log(1 - D(G(z))) ]*\n","\n","* x = real data instance\n","* E<sub>x</sub> = expected value over all real data instances.\n","* D(x) = discriminator's estimate of the probability that real data instance is real.\n","* z = noise\n","* G(z) = fake data instance, i.e. generator's output when given noise z.\n","* E<sub>z</sub> is the expected value over all fake instances.\n","* D(G(z)) = discriminator's estimate of the probability that a fake instance is real.\n","\n","The **generator** can't directly affect the *log(D(x))*. It can only **minimize *log(1 - D(G(z)))***\n","\n","### Modified Minimax Loss\n","\n","The Minimax Loss can cause the GAN to get stuck in the early stages of GAN training when the discriminator's job is very easy.<br>\n","So, modifying the generator loss so that the **generator** tries to **maximize *log D(G(z))***.\n","\n","### Wasserstein loss\n","\n","* A modification of the GAN scheme called *Wasserstein GAN* or *WGAN*.\n","* The weights throughout the GAN are clipped so that they remain within a constrained range.\n","* The discriminator does not actually classify instances. \n","  * So, it is called a *critic* instead of a discriminator.\n","* For each instance the discriminator outputs a number.\n","* Discriminator training tries to make the output bigger for real instances than for fake instances.\n","\n","Discriminator tries to maximize **Critic Loss**: *D(x) - D(G(z))*.\n","\n","Generator tries to maximize **Generator Loss**: D(G(z))\n","\n","Benefits:\n","* Less vulnerable to getting stuck than minimax-based GANs.\n","* Avoid problems with vanishing gradients.\n","* *Earth mover distance* between the real and generated distribution has the advantage of being a true metric.\n","  * A measure of distance in a space of probability distributions Cross-entropy is not a metric in this sense."]},{"cell_type":"markdown","metadata":{"id":"jQvhqS-7Em0P"},"source":["## GAN Common Problems\n","\n","### Vanishing Gradients\n","Problem:\n","* If your discriminator is too good, then generator training can fail due to vanishing gradients. \n","* An optimal discriminator doesn't provide enough information for the generator to make progress.\n","\n","Remedies:\n","  * *Wasserstein loss*\n","  * *Modified minimax loss*\n","\n","### Mode Collapse\n","Problem:\n","* If a generator produces an especially plausible output, the generator may learn to produce only that output.\n","* The generator always tries to find the one output that seems most plausible to the discriminator.\n","* If the generator starts producing the same output (or a small set of outputs) over and over again, the discriminator's best strategy is to learn to always reject that output. \n","* As the next generation of discriminator gets stuck in a local minimum and doesn't find the best strategy, then it's too easy for the next generator iteration to find the most plausible output for the current discriminator.\n","* Each iteration of generator over-optimizes for a particular discriminator, and the discriminator never manages to learn its way out of the trap. \n","* As a result, the generators rotate through a small set of output types.\n","\n","Remedies: force the generator to broaden its scope\n","* *Wasserstein loss*\n","  * Lets train the discriminator to optimality without worrying about vanishing gradients. If the discriminator doesn't get stuck in local minima, it learns to reject the outputs that the generator stabilizes on. \n","  * So the generator has to try something new.\n","* *Unrolled GANs*\n","  * Use a generator loss function that incorporates not only the current discriminator's classifications, but also the outputs of future discriminator versions. \n","  * So the generator can't over-optimize for a single discriminator.\n","\n","### Failure to Converge\n","Problem:\n","* The GAN may continue training past the point when the discriminator is giving completely random feedback.\n","* The generator starts to train on junk feedback, and its own quality may collapse.\n","* GANs frequently fail to converge.\n","\n","Remedies:\n","* Adding noise to discriminator inputs\n","* Regularization by Penalizing discriminator weights"]},{"cell_type":"markdown","metadata":{"id":"qWk76Ic0ICnm"},"source":["## GAN Variations\n","\n","### Progressive GANs\n","* The generator's first layers produce very low resolution images.\n","* The subsequent layers add details. \n","* This technique allows the GAN to train more quickly.\n","* Produces higher resolution images.\n","\n","### Conditional GANs\n","* Trains on a labeled data set.\n","* Lets you specify the label for each instance to be generated. \n","* Model the conditional probability P(X|Y).\n","\n","```\n","An unconditional MNIST GAN would produce random digits, \n","while a conditional MNIST GAN would let you specify which digit the GAN should generate.\n","```\n","\n","### Image-to-Image Translation\n","* Take an image as input.\n","* Map it to a generated output image with different properties. \n","* The loss is a weighted combination of the *discriminator loss* and a *pixel-wise loss*\n","* Penalizes the generator for departing from the source image.\n","\n","```\n","We can take a mask image with blob of color in the shape of a car, \n","and the GAN can fill in the shape with photorealistic car details.\n","```\n","```\n","Take sketches of handbags and turn them into photorealistic images of handbags.\n","```\n","\n","### CycleGAN\n","* Transform images from one set into images that could plausibly belong to another set. \n","* The training data is simply two sets of images. \n","* The system requires no labels or pairwise correspondences between images.\n","\n","```\n","Take an image of a horse and turn it into an image of a zebra.\n","```\n","\n","### Text-to-Image Synthesis\n","* Take text as input.\n","* Produce images that are plausible and described by the text. \n","* Can only produce images from a small set of classes.\n","\n","```\n","\"This flower has petals that are yellow with shades of orange.\" \n","produces an image of a flower that has yellow petals with orange shades.\n","```\n","\n","### Super-resolution\n","* Increase the resolution of images.\n","* Adds detail where necessary to fill in blurry areas.\n","* Some patterns might be skipped and made-up patterns might be produced.\n","\n","```\n","Given the blurry image, a sharper image is produced.\n","```\n","\n","### Face Inpainting\n","* Semantic image inpainting task. \n","* Chunks of an image are blacked out.\n","* The system tries to fill in the missing chunks.\n","\n","### Text-to-Speech\n","* Produce synthesized speech from text input.\n","\n","\n","\n"]}]}